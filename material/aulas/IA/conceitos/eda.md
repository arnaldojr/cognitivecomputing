# An√°lise Explorat√≥ria de Dados (EDA)

## Introdu√ß√£o

A **An√°lise Explorat√≥ria de Dados (EDA - Exploratory Data Analysis)** √© um processo cr√≠tico em ci√™ncia de dados que envolve a investiga√ß√£o sistem√°tica de conjuntos de dados para descobrir padr√µes, detectar anomalias, testar hip√≥teses e verificar suposi√ß√µes atrav√©s de estat√≠sticas descritivas e representa√ß√µes gr√°ficas.

> "Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise." - John Tukey

## Objetivos da EDA

### 1. Compreens√£o dos Dados
- Entender a estrutura e caracter√≠sticas dos dados
- Identificar tipos de vari√°veis (num√©ricas, categ√≥ricas, temporais)
- Verificar qualidade e integridade dos dados

### 2. Detec√ß√£o de Padr√µes
- Identificar tend√™ncias, sazonalidades e ciclos
- Descobrir relacionamentos entre vari√°veis
- Encontrar grupos ou clusters naturais nos dados

### 3. Identifica√ß√£o de Anomalias
- Detectar outliers e valores at√≠picos
- Identificar inconsist√™ncias e erros nos dados
- Verificar distribui√ß√µes incomuns

### 4. Formula√ß√£o de Hip√≥teses
- Gerar quest√µes de pesquisa baseadas nos dados
- Definir dire√ß√µes para an√°lises mais profundas
- Validar ou refutar suposi√ß√µes iniciais

## Etapas da EDA

### Fase 1: Conhecimento Inicial dos Dados

#### Vis√£o Geral
```python
import pandas as pd
import numpy as np

# Carregar dados
df = pd.read_csv('dataset.csv')

# Informa√ß√µes b√°sicas
print(f"Dimens√µes: {df.shape}")
print(f"Colunas: {list(df.columns)}")
print(f"Tipos de dados:\n{df.dtypes}")
print(f"Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

#### Primeiras Impress√µes
```python
# Primeiras e √∫ltimas linhas
print("Primeiras 5 linhas:")
print(df.head())

print("\n√öltimas 5 linhas:")
print(df.tail())

# Amostra aleat√≥ria
print("\nAmostra aleat√≥ria:")
print(df.sample(5))
```

### Fase 2: An√°lise da Qualidade dos Dados

#### Valores Ausentes
```python
# An√°lise de valores ausentes
missing_data = df.isnull().sum()
missing_percent = (missing_data / len(df)) * 100
missing_summary = pd.DataFrame({
    'Coluna': missing_data.index,
    'Valores_Ausentes': missing_data.values,
    'Porcentagem': missing_percent.values
}).sort_values('Porcentagem', ascending=False)

print(missing_summary[missing_summary['Valores_Ausentes'] > 0])
```

#### Duplicatas e Inconsist√™ncias
```python
# Verificar duplicatas
print(f"Linhas duplicadas: {df.duplicated().sum()}")

# Verificar inconsist√™ncias em dados categ√≥ricos
for col in df.select_dtypes(include=['object']).columns:
    unique_values = df[col].unique()
    print(f"\n{col}: {len(unique_values)} valores √∫nicos")
    if len(unique_values) < 20:  # Mostrar apenas se poucos valores
        print(f"Valores: {unique_values}")
```

#### Outliers Univariados
```python
def detect_outliers_iqr(df, column):
    """Detecta outliers usando m√©todo IQR"""
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers, lower_bound, upper_bound

# Aplicar para colunas num√©ricas
numeric_columns = df.select_dtypes(include=[np.number]).columns
for col in numeric_columns:
    outliers, lower, upper = detect_outliers_iqr(df, col)
    print(f"{col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.2f}%)")
```

### Fase 3: An√°lise Univariada

#### Vari√°veis Num√©ricas
```python
# Estat√≠sticas descritivas detalhadas
desc_stats = df.describe(include='all').T
desc_stats['missing'] = df.isnull().sum()
desc_stats['missing_pct'] = (desc_stats['missing'] / len(df)) * 100
print(desc_stats)

# Estat√≠sticas adicionais
for col in numeric_columns:
    data = df[col].dropna()
    print(f"\n{col}:")
    print(f"  Assimetria (Skewness): {data.skew():.3f}")
    print(f"  Curtose (Kurtosis): {data.kurtosis():.3f}")
    print(f"  Coeficiente de Varia√ß√£o: {(data.std() / data.mean()):.3f}")
```

#### Vari√°veis Categ√≥ricas
```python
# An√°lise de frequ√™ncias
categorical_columns = df.select_dtypes(include=['object', 'category']).columns

for col in categorical_columns:
    print(f"\n=== {col} ===")
    freq_table = df[col].value_counts()
    freq_percent = df[col].value_counts(normalize=True) * 100
    
    freq_summary = pd.DataFrame({
        'Frequ√™ncia': freq_table,
        'Porcentagem': freq_percent
    })
    print(freq_summary.head(10))
    
    # Verificar cardinalidade
    cardinality = df[col].nunique()
    print(f"Cardinalidade: {cardinality}")
    if cardinality > 50:
        print("‚ö†Ô∏è Alta cardinalidade - considere agrupamento")
```

### Fase 4: An√°lise Bivariada

#### Correla√ß√µes
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Matriz de correla√ß√£o
correlation_matrix = df[numeric_columns].corr()

# Identificar correla√ß√µes fortes
def find_strong_correlations(corr_matrix, threshold=0.7):
    """Encontra pares de vari√°veis com correla√ß√£o forte"""
    strong_corr = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = abs(corr_matrix.iloc[i, j])
            if corr_value > threshold:
                strong_corr.append({
                    'var1': corr_matrix.columns[i],
                    'var2': corr_matrix.columns[j],
                    'correlation': corr_matrix.iloc[i, j]
                })
    return pd.DataFrame(strong_corr).sort_values('correlation', key=abs, ascending=False)

strong_correlations = find_strong_correlations(correlation_matrix)
print("Correla√ß√µes fortes (|r| > 0.7):")
print(strong_correlations)
```

#### An√°lise por Grupos
```python
# Se houver vari√°vel target categ√≥rica
if 'target' in df.columns:
    target_col = 'target'
    
    # Estat√≠sticas por grupo
    for col in numeric_columns:
        if col != target_col:
            group_stats = df.groupby(target_col)[col].describe()
            print(f"\n{col} por {target_col}:")
            print(group_stats)
            
    # Teste estat√≠stico (exemplo: ANOVA)
    from scipy import stats
    
    for col in numeric_columns:
        if col != target_col:
            groups = [group[col].dropna() for name, group in df.groupby(target_col)]
            if len(groups) > 1:
                f_stat, p_value = stats.f_oneway(*groups)
                print(f"\n{col} - ANOVA: F={f_stat:.3f}, p={p_value:.3f}")
```

### Fase 5: An√°lise Multivariada

#### An√°lise de Componentes Principais (PCA)
```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Preparar dados para PCA
numeric_data = df[numeric_columns].dropna()
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_data)

# Aplicar PCA
pca = PCA()
pca_result = pca.fit_transform(scaled_data)

# Vari√¢ncia explicada
explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

print("Vari√¢ncia explicada por componente:")
for i, (individual, cumulative) in enumerate(zip(explained_variance, cumulative_variance)):
    print(f"PC{i+1}: {individual:.3f} (Cumulativa: {cumulative:.3f})")

# N√∫mero de componentes para 90% da vari√¢ncia
n_components_90 = np.argmax(cumulative_variance >= 0.9) + 1
print(f"\nComponentes necess√°rios para 90% da vari√¢ncia: {n_components_90}")
```

## Padr√µes Comuns e Red Flags

### Distribui√ß√µes Problem√°ticas

#### Alta Assimetria
```python
def analyze_skewness(df, threshold=2):
    """Analisa assimetria das vari√°veis num√©ricas"""
    skewed_features = []
    for col in df.select_dtypes(include=[np.number]).columns:
        skew_value = df[col].skew()
        if abs(skew_value) > threshold:
            skewed_features.append({
                'feature': col,
                'skewness': skew_value,
                'interpretation': 'Positiva' if skew_value > 0 else 'Negativa'
            })
    return pd.DataFrame(skewed_features)

skewed_analysis = analyze_skewness(df)
print("Vari√°veis com alta assimetria:")
print(skewed_analysis)
```

#### Transforma√ß√µes Sugeridas
```python
# Sugest√µes de transforma√ß√£o baseadas na assimetria
def suggest_transformations(skewness):
    """Sugere transforma√ß√µes baseadas na assimetria"""
    abs_skew = abs(skewness)
    if abs_skew < 0.5:
        return "Normal - sem transforma√ß√£o necess√°ria"
    elif abs_skew < 1:
        return "Ligeiramente assim√©trica - considere sqrt ou log"
    elif abs_skew < 2:
        return "Moderadamente assim√©trica - use log ou Box-Cox"
    else:
        return "Altamente assim√©trica - use log, Box-Cox ou Yeo-Johnson"

for _, row in skewed_analysis.iterrows():
    suggestion = suggest_transformations(row['skewness'])
    print(f"{row['feature']}: {suggestion}")
```

### Detec√ß√£o de Padr√µes Temporais

```python
# Se houver coluna de data
def analyze_temporal_patterns(df, date_col, value_col):
    """Analisa padr√µes temporais nos dados"""
    df_temp = df.copy()
    df_temp[date_col] = pd.to_datetime(df_temp[date_col])
    df_temp = df_temp.sort_values(date_col)
    
    # Extrair componentes temporais
    df_temp['year'] = df_temp[date_col].dt.year
    df_temp['month'] = df_temp[date_col].dt.month
    df_temp['day_of_week'] = df_temp[date_col].dt.day_of_week
    df_temp['hour'] = df_temp[date_col].dt.hour
    
    # An√°lise por per√≠odo
    patterns = {}
    patterns['yearly'] = df_temp.groupby('year')[value_col].mean()
    patterns['monthly'] = df_temp.groupby('month')[value_col].mean()
    patterns['weekly'] = df_temp.groupby('day_of_week')[value_col].mean()
    
    return patterns

# Exemplo de uso
# if 'date' in df.columns and 'sales' in df.columns:
#     temporal_patterns = analyze_temporal_patterns(df, 'date', 'sales')
```

## Checklist de EDA

### ‚úÖ Checklist B√°sico

1. **Conhecimento dos Dados**
   - [ ] Dimens√µes do dataset
   - [ ] Tipos de vari√°veis
   - [ ] Primeiras observa√ß√µes

2. **Qualidade dos Dados**
   - [ ] Valores ausentes identificados
   - [ ] Duplicatas verificadas
   - [ ] Inconsist√™ncias analisadas
   - [ ] Outliers detectados

3. **An√°lise Univariada**
   - [ ] Estat√≠sticas descritivas calculadas
   - [ ] Distribui√ß√µes analisadas
   - [ ] Frequ√™ncias de categorias verificadas

4. **An√°lise Bivariada**
   - [ ] Correla√ß√µes calculadas
   - [ ] Relacionamentos principais identificados
   - [ ] An√°lise por grupos realizada

5. **Insights e Pr√≥ximos Passos**
   - [ ] Padr√µes principais documentados
   - [ ] Hip√≥teses formuladas
   - [ ] Estrat√©gias de tratamento definidas

### üéØ Perguntas-Chave para Cada Etapa

#### Conhecimento Inicial
- Qual √© o tamanho do dataset?
- Que tipos de vari√°veis temos?
- Os dados fazem sentido do ponto de vista do neg√≥cio?

#### Qualidade
- H√° valores ausentes significativos?
- Existem inconsist√™ncias √≥bvias?
- Os outliers s√£o erros ou casos extremos v√°lidos?

#### Padr√µes
- Quais vari√°veis s√£o mais importantes?
- Existem grupos naturais nos dados?
- H√° relacionamentos n√£o-lineares?

#### A√ß√£o
- Que transforma√ß√µes s√£o necess√°rias?
- Quais vari√°veis podem ser removidas?
- Que an√°lises adicionais s√£o recomendadas?

## Ferramentas Automatizadas de EDA

### Pandas Profiling
```python
# pip install pandas-profiling
from pandas_profiling import ProfileReport

# Gerar relat√≥rio autom√°tico
profile = ProfileReport(df, title="EDA Autom√°tica", explorative=True)
profile.to_file("eda_report.html")
```

### Sweetviz
```python
# pip install sweetviz
import sweetviz as sv

# An√°lise completa
report = sv.analyze(df)
report.show_html("sweetviz_report.html")

# Compara√ß√£o entre datasets
# report = sv.compare([df_train, "Treino"], [df_test, "Teste"])
```

### AutoViz
```python
# pip install autoviz
from autoviz.AutoViz_Class import AutoViz_Class

AV = AutoViz_Class()
dft = AV.AutoViz("dataset.csv")
```

## Documenta√ß√£o e Comunica√ß√£o

### Template de Relat√≥rio EDA
```markdown
# Relat√≥rio de An√°lise Explorat√≥ria de Dados

## 1. Resumo Executivo
- Principais descobertas
- Recomenda√ß√µes imediatas
- Pr√≥ximos passos

## 2. Vis√£o Geral dos Dados
- Fonte e contexto
- Dimens√µes e estrutura
- Qualidade geral

## 3. An√°lise Detalhada
- Distribui√ß√µes das vari√°veis
- Relacionamentos identificados
- Padr√µes e anomalias

## 4. Insights de Neg√≥cio
- Implica√ß√µes pr√°ticas
- Oportunidades identificadas
- Riscos e limita√ß√µes

## 5. Recomenda√ß√µes T√©cnicas
- Tratamentos necess√°rios
- Vari√°veis para feature engineering
- Estrat√©gias de modelagem
```

### Boas Pr√°ticas de Comunica√ß√£o

1. **Use Visualiza√ß√µes Efetivas**
   - Escolha o gr√°fico certo para cada tipo de dado
   - Mantenha simplicidade e clareza
   - Adicione contexto e interpreta√ß√£o

2. **Conte uma Hist√≥ria**
   - Organize insights de forma l√≥gica
   - Conecte descobertas com objetivos de neg√≥cio
   - Use linguagem acess√≠vel ao p√∫blico-alvo

3. **Seja Honesto sobre Limita√ß√µes**
   - Mencione dados ausentes significativos
   - Identifique potenciais vieses
   - Sugira valida√ß√µes adicionais

A EDA √© uma arte que combina t√©cnica estat√≠stica, intui√ß√£o de neg√≥cio e pensamento cr√≠tico. Quanto mais voc√™ pratica, mais eficiente se torna em extrair insights valiosos dos dados.
